# 栈溢出首页爬取
> 对于栈溢出首页开发出的的较为完善的爬虫，在搭建网安大模型语料库的过程中，需要爬取一些问答数据，考虑到栈溢出中涉及较多，于是完成该部分功能开发工作，代码已开源，拉取后可直接使用。

# 爬虫脚本功能分析crwal.py

## 1. 导入库
- `requests`: 用于发送 HTTP 请求和处理响应。
- `BeautifulSoup`: 用于解析 HTML 页面。
- `csv`: 用于读写 CSV 文件。
- `signal`: 用于处理终止信号。
- `time`: 用于添加延时。
- `os`: 用于文件操作。
- `concurrent.futures.ThreadPoolExecutor`: 用于并发处理任务。

## 2. 全局变量
- `stop_crawling`: 用于标记是否停止爬取。

## 3. 信号处理函数
- `signal_handler`: 处理 SIGINT 信号（例如用户按下 Ctrl+C），将 `stop_crawling` 设置为 `True` 并打印停止信息。

## 4. 函数定义

### `fetch_page(url)`
- 功能: 发送 HTTP GET 请求获取页面内容。
- 输入: `url`（目标网页的 URL）
- 输出: 页面 HTML 内容（字符串）或 `None`（请求失败）。

### `parse_question_page(html)`
- 功能: 解析问题页面的 HTML 内容，提取问题详细描述、标签、发布日期和回答内容。
- 输入: 页面 HTML 内容（字符串）
- 输出: 问题数据字典和回答数据列表。

### `parse_page(html)`
- 功能: 解析问题列表页面的 HTML 内容，提取每个问题的标题、链接、投票数、回答数和浏览量。
- 输入: 页面 HTML 内容（字符串）
- 输出: 问题数据列表（字典）。

### `process_question(question_data)`
- 功能: 处理每个问题的数据，获取详细信息和回答内容。
- 输入: 问题数据字典（包含问题的链接）
- 输出: 更新后的问题数据字典，包含详细描述、标签、发布日期和回答内容。

### `save_to_csv(data, filename='stackoverflow_data4000_6000.csv')`
- 功能: 将数据保存到 CSV 文件中。
- 输入: 数据列表（字典），文件名（可选）
- 输出: 无

### `load_last_page(file='last_page.txt')`
- 功能: 从文件中加载最后爬取的页码和最大页码。
- 输入: 文件名（可选）
- 输出: 最后爬取的页码和最大页码。

### `save_last_page(page, max_page, file='last_page.txt')`
- 功能: 将当前页码和最大页码保存到文件中。
- 输入: 当前页码、最大页码、文件名（可选）
- 输出: 无

## 5. `main` 函数
- 功能: 主爬取逻辑，包括分页爬取、数据解析和保存。
- 步骤:
  1. 加载最后爬取的页码和最大页码。
  2. 循环遍历每一页，发送请求并解析问题列表。
  3. 使用线程池并发处理每个问题，获取详细信息。
  4. 将数据保存到 CSV 文件中。
  5. 处理终止信号，保存数据并停止爬取。

## 6. 程序入口
- 检查 CSV 文件是否存在，若不存在则创建并写入表头。
- 调用 `main` 函数开始爬取。

## 7. 特点
- 支持中断爬取（使用 `SIGINT` 信号）。
- 使用线程池提高处理效率。
- 自动保存和加载爬取进度。
- 处理请求错误和页面解析失败的情况。
- 每个问题的所有回答被保存到 CSV 文件中。

# CSV 处理脚本功能分析process.py

## 1. 导入库
- `csv`: 用于读写 CSV 文件。
- `tqdm`: 用于显示进度条。

## 2. 函数定义

### `process_csv(input_filename, output_filename)`
- 功能: 处理 CSV 文件，筛选出接受的回答，并将结果写入新的 CSV 文件。
- 输入: 
  - `input_filename`: 输入 CSV 文件名
  - `output_filename`: 输出 CSV 文件名
- 输出: 无

#### 步骤:
1. **设置字段大小限制**:
   - 增加 CSV 字段的大小限制，防止字段内容过长导致的错误。
   - 使用 `csv.field_size_limit(max_field_size)` 设置字段大小限制为 10MB。

2. **读取输入 CSV 文件**:
   - 打开输入 CSV 文件并读取其内容。
   - 使用 `csv.DictReader` 读取每一行，将其作为字典存储。
   - 统计行数以便于显示进度条。
   - 重置文件指针到文件开头以便重新读取数据。

3. **创建输出 CSV 文件**:
   - 打开输出 CSV 文件以进行写操作。
   - 使用 `csv.DictWriter` 创建一个写入器，并写入表头。

4. **处理和筛选数据**:
   - 使用 `tqdm` 显示进度条，遍历每一行。
   - 筛选出 `'accepted'` 列值为 `'是'` 的行，并将这些行写入输出文件。

5. **完成消息**:
   - 打印一条消息，指示筛选数据已成功写入输出文件。

## 3. 程序入口
- **文件名设置**:
  - `input_csv` 变量指定输入 CSV 文件名。
  - `output_csv` 变量指定输出 CSV 文件名。

- **函数调用**:
  - 调用 `process_csv` 函数进行数据处理。

## 4. 特点
- **字段大小限制**: 通过设置字段大小限制来处理较长的字段内容。
- **进度条**: 使用 `tqdm` 显示处理进度。
- **数据筛选**: 仅保存 `'accepted'` 列为 `'是'` 的记录。
- **文件操作**: 支持大文件处理，通过重新读取文件来统计行数并进行处理。


# 使用说明

由于是由于长时间爬取并仍需测试，没有将参数放入命令行导入，直接在脚本中修改需要处理的文件即可，lastpage中记录目前爬取到的页面数字，可以随时终止爬虫，下次启动时自动顺着上次位置爬取。脚本在命令行给出了充足调试信息。对于爬取的内容，直接使用process.py即可进行处理，作用是筛选出被接受的答案。

# CSV 列字段分析

## 列字段

1. **title**
   - **描述**: 问题的标题。
   - **类型**: 字符串
   - **示例**: `"如何在 Python 中处理异常？"`

2. **link**
   - **描述**: 问题的链接，指向 Stack Overflow 页面。
   - **类型**: 字符串
   - **示例**: `"https://stackoverflow.com/questions/12345678/example-question"`

3. **votes**
   - **描述**: 问题的投票数。
   - **类型**: 字符串（通常是数字）
   - **示例**: `"15"`

4. **answers**
   - **描述**: 问题的回答数。
   - **类型**: 字符串（通常是数字）
   - **示例**: `"3"`

5. **views**
   - **描述**: 问题的浏览量。
   - **类型**: 字符串（通常是数字）
   - **示例**: `"1200"`

6. **description**
   - **描述**: 问题的详细描述内容。
   - **类型**: 字符串
   - **示例**: `"我在使用 Python 时遇到异常处理的问题。如何捕获并处理异常？"`

7. **tags**
   - **描述**: 问题的标签列表，描述问题的主题。
   - **类型**: 字符串（标签之间用逗号分隔）
   - **示例**: `"python, exception-handling"`

8. **date**
   - **描述**: 问题的发布日期。
   - **类型**: 字符串（日期格式）
   - **示例**: `"2023-05-01"`

9. **answer_content**
   - **描述**: 回答的内容。
   - **类型**: 字符串
   - **示例**: `"你可以使用 `try-except` 块来捕获异常。例子如下： ..."`

10. **accepted**
    - **描述**: 标记回答是否被接受。
    - **类型**: 字符串（通常是 `"是"` 或 `"否"`）
    - **示例**: `"是"`

# 结尾

感谢您查看我们的脚本。这些字段提供了对 Stack Overflow 问题和回答的详细描述，方便您进行进一步的数据分析和处理。

## 使用须知

- **数据来源**: 本数据源自 Stack Overflow，使用数据和爬虫时请遵守 Stack Overflow 的使用条款和隐私政策。
- **数据用途**: 本脚本用于分析和研究目的，禁止用于任何商业用途或违反法律法规的行为。
- **数据准确性**: 我们努力确保脚本的功能，但不对数据的完整性和时效性作任何保证。
- **数据更新**: 脚本可能会定期更新，请关注项目页面以获取最新功能。


## 依赖环境

为了顺利运行本项目，请确保您的环境中安装了以下依赖库：

- **Python 版本**: 3.x
- **主要依赖库**:
  - `requests`: 用于发送 HTTP 请求
  - `beautifulsoup4`: 用于解析 HTML 内容
  - `tqdm`: 用于显示进度条
  - `csv`: 用于读写 CSV 文件（Python 内置库）

您可以使用 `pip` 安装这些依赖库，命令如下：

```bash
pip install requests beautifulsoup4 tqdm
```

## 许可证

本项目遵循 [MIT 许可证](LICENSE)。您可以自由使用、修改和分发本项目的代码和数据，但需要遵循许可证中的条款。

如有任何问题或建议，欢迎在 GitHub 上提出 issue 或通过其他方式与我们联系。感谢您的支持和贡献！

- 项目团队 B503大模型项目组
